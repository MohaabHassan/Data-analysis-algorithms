{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 [L-fold Cross Validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the second question, we are required to implement a L-fold cross validation function for the KNN regressor implemented in question 1 of the assignment. L-fold cross validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called 'L' that refers to the number of groups that a given data sample is to be split into. \n",
    "\n",
    "#### We begin with loading the data from the previous question and the knn function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data <- read.csv('Task1A_train.csv')\n",
    "data_test <- read.csv('Task1A_test.csv')\n",
    "training_data <- data['x1']\n",
    "training_label <- data['y']\n",
    "test_data <- data_test['x1']\n",
    "test_label <- data_test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN function from question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN function (distance should be one of euclidean, maximum, manhattan, canberra, binary or minkowski)\n",
    "knn <- function(train.data, train.label, test.data, K, distance = 'euclidean'){\n",
    "  ## count number of train samples\n",
    "  train.len <- nrow(train.data)\n",
    "  \n",
    "  ## count number of test samples\n",
    "  test.len <- nrow(test.data)\n",
    "  \n",
    "  ## calculate distances between samples\n",
    "  dist <- as.matrix(dist(rbind(test.data, train.data), method= distance))[1:test.len, (test.len+1):(test.len+train.len)]\n",
    "  \n",
    "  ## for each test sample...\n",
    "  test.label<- c()\n",
    "  #test.data <- data.frame(test.data)\n",
    "  #test.data<-data.frame(x1 = c(test.data))\n",
    "  \n",
    "  \n",
    "  for (i in 1:test.len){\n",
    "    ### ...find its K nearest neighbours from training sampels...\n",
    "    nn <- as.data.frame(sort(dist[i,], index.return = TRUE))[1:K,2]\n",
    "    \n",
    "    ###... and calculate the predicted labels according to the majority vote\n",
    "    test.label[i]<- (mean(train.label[nn,1]))\n",
    "  }\n",
    "  \n",
    "  ## return the class labels as output\n",
    "  return (test.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement the L-fold cross validation function. The function takes 4 parameters which is the training data, the training label, number of neighbors 'k' and the number of folds i.e. numFold (L). The function does the following steps,\n",
    "- Creates a dataframe to store the errors for different values of k.\n",
    "- Creates L equally sized folds of the data.\n",
    "- For each fold, subset the data to training and testing and run knn and record the error on these folds.\n",
    "- We then return the mean of these errors back to the miss dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv<- function(train.data, train.label, K, numFold=10){\n",
    "  #Randomly shuffle the data\n",
    "  miss <- data.frame('K'=1:10, 'Error'=rep(0,10))\n",
    "  #train.data<-train.data[sample(nrow(train.data)),]\n",
    "  \n",
    "  #Create 10 equally size folds\n",
    "  folds <- cut(seq(1,nrow(train.data)),breaks=numFold,labels=FALSE)\n",
    "  \n",
    "  #Perform 10 fold cross validation\n",
    "  for(i in 1:numFold){\n",
    "    #Segement your data by fold using the which() function \n",
    "    testIndexes <- which(folds==i,arr.ind=TRUE)\n",
    "    testData <- train.data[testIndexes, ]\n",
    "    testLabel<- train.label[testIndexes, ]\n",
    "    trainData <- train.data[-testIndexes, ]\n",
    "    trainLabel<- train.label[-testIndexes, ]\n",
    "    trainData<-data.frame(trainData)\n",
    "    colnames(trainData) <- c(\"x1\")\n",
    "    trainLabel<-data.frame(y<-trainLabel)\n",
    "    colnames(trainLabel) <- c(\"y\")\n",
    "    testData<-data.frame(x1<-testData)\n",
    "    colnames(testData) <- c(\"x1\")\n",
    "    knn(trainData,trainLabel,trainData,K,distance='euclidean')\n",
    "    miss[i,'Error'] <-  sqrt(sum((testLabel - knn(trainData, trainLabel, testData, K=K))**2)/nrow(testData))\n",
    "    \n",
    "  }\n",
    "  return(mean(miss$Error))\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below stores the test error for 1.2....15 values of K by passing them through the cross validation function. As in KNN, we use RMSE here as well for our error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss <- data.frame('K'=1:15, 'Error'=rep(0,15))\n",
    "for(K in 1:15)\n",
    "{\n",
    "  miss[K,'Error']=cv(training_data,training_label,K,10)\n",
    "\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss.m <- melt(miss, id='K') # reshape for visualization\n",
    "names(miss.m) <- c('K', 'type', 'RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>K</th><th scope=col>type</th><th scope=col>RMSE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1       </td><td>Error    </td><td>0.9064516</td></tr>\n",
       "\t<tr><td> 2       </td><td>Error    </td><td>0.9248274</td></tr>\n",
       "\t<tr><td> 3       </td><td>Error    </td><td>1.0076324</td></tr>\n",
       "\t<tr><td> 4       </td><td>Error    </td><td>0.9307078</td></tr>\n",
       "\t<tr><td> 5       </td><td>Error    </td><td>0.9602899</td></tr>\n",
       "\t<tr><td> 6       </td><td>Error    </td><td>0.9295822</td></tr>\n",
       "\t<tr><td> 7       </td><td>Error    </td><td>1.0283564</td></tr>\n",
       "\t<tr><td> 8       </td><td>Error    </td><td>0.9793313</td></tr>\n",
       "\t<tr><td> 9       </td><td>Error    </td><td>1.0941474</td></tr>\n",
       "\t<tr><td>10       </td><td>Error    </td><td>1.0688254</td></tr>\n",
       "\t<tr><td>11       </td><td>Error    </td><td>1.1758569</td></tr>\n",
       "\t<tr><td>12       </td><td>Error    </td><td>1.1542295</td></tr>\n",
       "\t<tr><td>13       </td><td>Error    </td><td>1.2783540</td></tr>\n",
       "\t<tr><td>14       </td><td>Error    </td><td>1.2742742</td></tr>\n",
       "\t<tr><td>15       </td><td>Error    </td><td>1.4097080</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " K & type & RMSE\\\\\n",
       "\\hline\n",
       "\t  1        & Error     & 0.9064516\\\\\n",
       "\t  2        & Error     & 0.9248274\\\\\n",
       "\t  3        & Error     & 1.0076324\\\\\n",
       "\t  4        & Error     & 0.9307078\\\\\n",
       "\t  5        & Error     & 0.9602899\\\\\n",
       "\t  6        & Error     & 0.9295822\\\\\n",
       "\t  7        & Error     & 1.0283564\\\\\n",
       "\t  8        & Error     & 0.9793313\\\\\n",
       "\t  9        & Error     & 1.0941474\\\\\n",
       "\t 10        & Error     & 1.0688254\\\\\n",
       "\t 11        & Error     & 1.1758569\\\\\n",
       "\t 12        & Error     & 1.1542295\\\\\n",
       "\t 13        & Error     & 1.2783540\\\\\n",
       "\t 14        & Error     & 1.2742742\\\\\n",
       "\t 15        & Error     & 1.4097080\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| K | type | RMSE |\n",
       "|---|---|---|\n",
       "|  1        | Error     | 0.9064516 |\n",
       "|  2        | Error     | 0.9248274 |\n",
       "|  3        | Error     | 1.0076324 |\n",
       "|  4        | Error     | 0.9307078 |\n",
       "|  5        | Error     | 0.9602899 |\n",
       "|  6        | Error     | 0.9295822 |\n",
       "|  7        | Error     | 1.0283564 |\n",
       "|  8        | Error     | 0.9793313 |\n",
       "|  9        | Error     | 1.0941474 |\n",
       "| 10        | Error     | 1.0688254 |\n",
       "| 11        | Error     | 1.1758569 |\n",
       "| 12        | Error     | 1.1542295 |\n",
       "| 13        | Error     | 1.2783540 |\n",
       "| 14        | Error     | 1.2742742 |\n",
       "| 15        | Error     | 1.4097080 |\n",
       "\n"
      ],
      "text/plain": [
       "   K  type  RMSE     \n",
       "1   1 Error 0.9064516\n",
       "2   2 Error 0.9248274\n",
       "3   3 Error 1.0076324\n",
       "4   4 Error 0.9307078\n",
       "5   5 Error 0.9602899\n",
       "6   6 Error 0.9295822\n",
       "7   7 Error 1.0283564\n",
       "8   8 Error 0.9793313\n",
       "9   9 Error 1.0941474\n",
       "10 10 Error 1.0688254\n",
       "11 11 Error 1.1758569\n",
       "12 12 Error 1.1542295\n",
       "13 13 Error 1.2783540\n",
       "14 14 Error 1.2742742\n",
       "15 15 Error 1.4097080"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "miss.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code generates the plot for test error various values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nFg0dcoeGAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(data=miss.m, aes(x=1/K, y=RMSE, color=type)) + geom_line() +\n",
    "  scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "  ggtitle(\"RMSE\")\n",
    "#knn(training_data,training_label,test_data,K=4,distance='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, with a small value of k, our training data error is very small due to overfitting of data due to very low bias. But as the size of k increases, the complexity of the model increases, which will lead to an increase on the error in the training data but will lead to a decrease of the error in test data. Also, if we use a very high value of 'k', as in the first question would lead to underfitting of the data leading to a high error on the training data again. The optimal value of k lies somewhere around 8 neighbors(i.e. 1/k = 0.25). This region is where we can find a low test region for the model fitted on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
