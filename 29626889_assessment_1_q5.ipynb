{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "#### In this part of the assignment, we are required to implement Stochastic and batch gradient descents with ridge regression. Ridge regression is basically a weight decay because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data (i.e., only big enough data should result to more complex model). We implement ridge regression with Stochastic and batch gradient descent as done below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.1\""
     ]
    }
   ],
   "source": [
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfunction to read the data in the desired format\n",
    "read_data <- function(fname, sc) {\n",
    "   data <- read.csv(file=fname,head=TRUE,sep=\",\")\n",
    "   nr = dim(data)[1]\n",
    "   nc = dim(data)[2]\n",
    "   x = data[1:nr,1:(nc-1)]\n",
    "   y = data[1:nr,nc]\n",
    "   if (isTRUE(sc)) {\n",
    "      x = scale(x)\n",
    "      y = scale(y)\n",
    "   }\n",
    "   \n",
    "   return (list(\"x\" = x, \"y\" = y))\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict labels based on  weight vectore\n",
    "predict_func <- function(Phi, w){\n",
    "    return(Phi%*%w)\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to calculate the objective function for the training\n",
    "train_obj_func <- function (Phi, w, label, lambda){\n",
    "    # the L2 regulariser is already included in the objective function for training \n",
    "    return(mean((predict_func(Phi, w) - label)^2) + .5 * lambda * w %*% w)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to compute the error of the model\n",
    "get_errors <- function(train_data, test_data, W) {\n",
    "   n_weights = dim(W)[1]\n",
    "   errors = matrix(,nrow=n_weights, ncol=2)\n",
    "   for (tau in 1:n_weights) {\n",
    "       #calculate train error \n",
    "      errors[tau,1] = train_obj_func(train_data$x, W[tau,],train_data$y, 0)\n",
    "       #calculate test error\n",
    "      errors[tau,2] = train_obj_func(test_data$x, W[tau,],test_data$y, 0)\n",
    "   }\n",
    "    #return train and test errors\n",
    "   return(errors)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement SGD \n",
    "sgd_train <- function(train_x, train_y, lambda, eta, epsilon, max_epoch) {\n",
    "\n",
    "   train_len = dim(train_x)[1]\n",
    "   tau_max = max_epoch * train_len\n",
    "\n",
    "   W <- matrix(,nrow=tau_max, ncol=ncol(train_x)) \n",
    "   W[1,] <- runif(ncol(train_x))\n",
    "  \n",
    "   tau = 1 # counter \n",
    "   obj_func_val <-matrix(,nrow=tau_max, ncol=1) \n",
    "   obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "\n",
    "   while (tau <= tau_max){\n",
    "\n",
    "       # check termination criteria\n",
    "       if (obj_func_val[tau,1]<=epsilon) {break}\n",
    " \n",
    "       # shuffle data:\n",
    "       train_index <- sample(1:train_len, train_len, replace = FALSE)\n",
    "    \n",
    "       # loop over each datapoint\n",
    "       for (i in train_index) {\n",
    "           # increment the counter\n",
    "           tau <- tau + 1\n",
    "           if (tau > tau_max) {break}\n",
    "\n",
    "           # make the weight update\n",
    "           y_pred <- predict_func(train_x[i,], W[tau-1,])\n",
    "           W[tau,] <- sgd_update_weight(W[tau-1,], train_x[i,], train_y[i], y_pred, lambda, eta)\n",
    "\n",
    "           # keep track of the objective funtion\n",
    "           obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "       }\n",
    "   }\n",
    "   # resulting values for the training objective function as well as the weights\n",
    "   return(list('vals'=obj_func_val,'W'=W))\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update for SGD - <br/>\n",
    "In order to update the weights in SGD, we first require to update the gradient followed by updating the weights. \n",
    "The new gradient becomes $$gradient = (matrix(y_true - y_pred)*x) + \\lambda*W_{prev}$$\n",
    "Hence, the weight update becomes $$W_{prev} - eta*gradient$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update the weights in SGD\n",
    "sgd_update_weight <- function(W_prev, x, y_true, y_pred, lambda, eta) {\n",
    "   # we update the gradient and weights below as mentioned above\n",
    "   gradient = matrix(- (y_true-y_pred) * x) +  lambda*W_prev\n",
    "   return(W_prev - eta * gradient)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgd_train <- function(train_x, train_y, lambda, eta, epsilon, max_epoch) {\n",
    "\n",
    "   train_len = dim(train_x)[1]\n",
    "\n",
    "   W <- matrix(,nrow=(max_epoch+1), ncol=ncol(train_x))\n",
    "   W[1,] <- runif(ncol(train_x))\n",
    "\n",
    "   tau = 1 # counter \n",
    "   obj_func_val <-matrix(,nrow=(max_epoch+1), ncol=1)\n",
    "   obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "\n",
    "   trainin_size = dim(train_x)[1]\n",
    "   for (tau in 1:max_epoch){\n",
    "\n",
    "       # check termination criteria\n",
    "       if (obj_func_val[tau,1]<=epsilon) {break}\n",
    "\n",
    "       # make prediction over the training set\n",
    "       y_pred = train_x %*% W[tau,]\n",
    "\n",
    "       # update the weight you may decide to chose linea search or not (we are not using line search here)\n",
    "       W[tau+1,] = bgd_update_weight(W[tau,], train_x, train_y, y_pred, lambda, eta)\n",
    "\n",
    "       # keep track of the objective funtion\n",
    "       obj_func_val[tau+1,1] = train_obj_func(train_x, W[tau+1,],train_y, lambda)\n",
    "       \n",
    "       \n",
    "   } \n",
    "   # resulting values for the training objective function as well as the weights\n",
    "   return(list('vals'=obj_func_val,'W'=W))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update for BGD - \n",
    "Below are the steps for updating weights in BGD. For a BGD, since we perform it over the entire data at a go, we take colMeans of the gradient and then multiply it with eta and subtract from the weight. This shown below,\n",
    "We update gradient as $$gradient= -colMeans(matrix((y_true-y_pred),nrow=dim(x)[1],ncol=dim(x)[2]) * x) + lambda*W_prev$$\n",
    "<br>\n",
    "so weight update will be $$W_{prev} = - eta * gradient$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update the weights in BGD\n",
    "bgd_update_weight <- function(W_prev, x, y_true, y_pred, lambda, eta) {\n",
    "   # update gradient by adding lambda*W_prev and perform the weight update\n",
    "   gradient = -colMeans(matrix((y_true-y_pred),nrow=dim(x)[1],ncol=dim(x)[2]) * x)+ lambda*W_prev\n",
    "   return (W_prev - eta * gradient)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now the load the dataset, initialize our parameters and the plot the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = read_data(\"Task1C_train.csv\", TRUE)\n",
    "dtest = read_data(\"Task1C_test.csv\", TRUE)\n",
    " \n",
    "  \n",
    "#intialize parameters\n",
    "   max_epoch = 18\n",
    "   epsilon = .001\n",
    "   eta = .01\n",
    "   lambda=0\n",
    "    #store weight updates\n",
    "    options(warn=-1)\n",
    " ##### SGD and BGD Trainng\n",
    "\n",
    "   train_res_sgd = sgd_train(dtrain$x, dtrain$y, lambda, eta, epsilon, max_epoch)\n",
    "    train_res_bgd = bgd_train(dtrain$x, dtrain$y, lambda, eta, epsilon, max_epoch-1)\n",
    "    #find error rate for each weight update iteration both test and train errors \n",
    "    # column one for test error and column two for training errors\n",
    "    errors_sgd = get_errors(dtrain, dtest, train_res_sgd$W) # get errors for SGD\n",
    "    errors_bgd = get_errors(dtrain, dtest, train_res_bgd$W) # get erros for BGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store the errors for BGD,for every N data points one point of BGD\n",
    "train = read.csv(\"Task1C_train.csv\", TRUE) # we re-read the dataset to get the lenght of the training data\n",
    "len.train<-nrow(train)\n",
    "miss<-c()\n",
    "for( val in errors_bgd[,1])\n",
    "{\n",
    "    start <- length(miss)\n",
    "   end <- start+ len.train\n",
    "    miss[end]<- val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store errors for BGD and SGD\n",
    "plotdataframe<-data.frame('x'=1:length(miss),'SGD'=errors_sgd[,1],'BGD'=miss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the errors for SGD and BGD on a single plot and compare their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAv8QzMzNNTU1o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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(data=plotdataframe,aes(x=x,y=SGD))+geom_point(aes(x=plotdataframe$x,y=plotdataframe$BGD,color = 'red'))+geom_line(aes(x=plotdataframe$x,y=plotdataframe$SGD,color= 'blue'))+ylim(0,1)+ylab('Error') + theme(legend.position = \"none\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line above represents the error for batch gradient descent and red line represents the error for stochastic gradient descent. From the above graph, we can ascertain that SGD converges much faster in comparison to BGD. This is due to the fact how these algorithms are designed. In SGD, we process each data point at a time while in batch, we process the entire dataset at a time which is computationally more costly. A thing to note though is in fluctuations of the error rate. In BGD, the error rate decreases in a stable fashion in comparison with SGD where there are multiple fluctuations in the error rate since we process each datapoint at a time. This is due to the fact that in sGD, the number of iterations is 18 x N number of times while in BGD, it fluctuates only 18 number of times.\n",
    "\n",
    "In addition to above, the stochastic gradient descent produces a lower error rate compared to batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
